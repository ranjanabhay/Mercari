{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1517195710.431419\n",
      "2018-01-29 03:15:10\n",
      "[10.1405770778656] Finished to load the data\n",
      "Shape of training Data (1482535, 8)\n",
      "Shape of testing Data (693359, 7)\n",
      "Fields of training dataset Index(['train_id', 'name', 'item_condition_id', 'category_name', 'brand_name',\n",
      "       'price', 'shipping', 'item_description'],\n",
      "      dtype='object')\n",
      "Fields of testing dataset Index(['test_id', 'name', 'item_condition_id', 'category_name', 'brand_name',\n",
      "       'shipping', 'item_description'],\n",
      "      dtype='object')\n",
      "Shape of training data: (2175894, 9)\n",
      "[20.432743072509766] Split categories complete and original dropped\n",
      "brand_name             object\n",
      "item_condition_id    category\n",
      "item_description       object\n",
      "name                   object\n",
      "price                 float64\n",
      "shipping                int64\n",
      "test_id               float64\n",
      "train_id              float64\n",
      "first_category       category\n",
      "second_category      category\n",
      "third_category       category\n",
      "dtype: object\n",
      "0           MLB Cincinnati Reds T Shirt Size XL\n",
      "1              Razer BlackWidow Chroma Keyboard\n",
      "2                                AVA-VIV Blouse\n",
      "3                         Leather Horse Statues\n",
      "4                          24K GOLD plated rose\n",
      "5              Bundled items requested for Ruie\n",
      "6            Acacia pacific tides santorini top\n",
      "7          Girls cheer and tumbling bundle of 7\n",
      "8                         Girls Nike Pro shorts\n",
      "9        Porcelain clown doll checker pants VTG\n",
      "10                              Smashbox primer\n",
      "11                       New vs pi k body mists\n",
      "12                           Black Skater dress\n",
      "13                         Sharpener and eraser\n",
      "14           HOLD for Dogs2016 Minnetonka boots\n",
      "15                  Sephora tarte birthday gift\n",
      "16                            Glitter Eyeshadow\n",
      "17          New: Baby K'tan active baby carrier\n",
      "18          Too Faced Limited \"Merry Macaroons\"\n",
      "19               Cream/ Beige Front Cross Shirt\n",
      "20              Torrid Nautical Peplum Tube Top\n",
      "21             NWT VS ULTIMATE SPORTS BRA 34ddd\n",
      "22               Galaxy S7 Edge (Unlocked) 32GB\n",
      "23                           Triple car charger\n",
      "24                   Black and Red Baseball Tee\n",
      "25                        Air Jordan carmine 6s\n",
      "26      Otterbox Defender iPhone 6 Plus/6s Plus\n",
      "27       LuLaRoe OS Black With White Polka Dots\n",
      "28            Forever21 floral romper strapless\n",
      "29                                Kendra bundle\n",
      "                         ...                   \n",
      "470                  Victoria's Secret Tote Bag\n",
      "471                            Nike track shoes\n",
      "472                      LuLaRoe NWT Amelia 3XL\n",
      "473                     Hold for Engravemybones\n",
      "474           Nightmare Before Christmas Hoodie\n",
      "475                        H&M womens XS hoodie\n",
      "476                  VS PINK full zip hoodie XS\n",
      "477             Vintage cyber pup jr. dog walks\n",
      "478           Tory Burch black leather flat 9.5\n",
      "479    Women's Converse All Star tennis shoes 7\n",
      "480              BAMBOO ADULT TOOTHBRUSH 2-PACK\n",
      "481        Blue Ombr√© Sequined Homecoming Dress\n",
      "482                 Peanut Butter Air Force One\n",
      "483                     UCLA sweatshirt - youth\n",
      "484      Maybelline Mascara #130 Blackest Black\n",
      "485                           Jordan Toddler 5c\n",
      "486                          Patagonia Tan Vest\n",
      "487             Lularoe tc pink camera leggings\n",
      "488        LA Girl Banana Yellow Setting Powder\n",
      "489       Adidas men's gray athletic top medium\n",
      "490        Lena Cosmetics Jason liquid lipstick\n",
      "491              Marvel legends Captain America\n",
      "492                  5 Disney Princess Dress Up\n",
      "493                Call Of Duty Black Ops 3 PS4\n",
      "494                   Caroline Pollack Necklace\n",
      "495                        Columbia Down Jacket\n",
      "496                          Women's latch belt\n",
      "497          Nintendo 64 Men's T shirt Size 2XL\n",
      "498                  Anastasia lipgloss - sepia\n",
      "499                                        Lush\n",
      "Name: name, Length: 500, dtype: object\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "[231.3576991558075] Count Vectorize categories completed.\n",
      "Normalize text\n",
      "Extract wordbags\n",
      "[425.2025089263916] Vectorize item_description completed.\n",
      "[438.45667123794556] Label Binarize brand name completed.\n",
      "[443.8770911693573] Get dummies on item condition and shipping done.\n",
      "(2175894, 6) (2175894, 2034186) (2175894, 4501) (2175894, 14) (2175894, 143) (2175894, 977)   (0, 504783)\t-1.5\n",
      "  (0, 488550)\t1.0\n",
      "  (0, 450022)\t-1.5\n",
      "  (0, 390634)\t-1.5\n",
      "  (0, 372203)\t-1.5\n",
      "  (0, 298745)\t-1.5\n",
      "  (0, 242255)\t-1.0\n",
      "  (0, 211928)\t-1.0\n",
      "  (0, 144615)\t1.0\n",
      "  (0, 61061)\t-1.5\n",
      "  (0, 20100)\t1.0\n",
      "  (1, 500329)\t1.0\n",
      "  (1, 454608)\t-1.5\n",
      "  (1, 393542)\t-1.5\n",
      "  (1, 377130)\t-1.0\n",
      "  (1, 353939)\t-1.0\n",
      "  (1, 191096)\t1.5\n",
      "  (1, 128928)\t-1.5\n",
      "  (2, 365064)\t1.5\n",
      "  (2, 109910)\t-1.0\n",
      "  (2, 80538)\t1.5\n",
      "  (2, 79994)\t1.5\n",
      "  (3, 478680)\t-1.5\n",
      "  (3, 386262)\t1.5\n",
      "  (3, 283221)\t-1.5\n",
      "  :\t:\n",
      "  (2175891, 282067)\t-1.5\n",
      "  (2175891, 242978)\t1.5\n",
      "  (2175891, 196822)\t-1.5\n",
      "  (2175891, 158898)\t1.0\n",
      "  (2175891, 154321)\t-1.0\n",
      "  (2175891, 103321)\t1.0\n",
      "  (2175891, 81683)\t1.0\n",
      "  (2175891, 42494)\t1.5\n",
      "  (2175891, 33337)\t1.5\n",
      "  (2175892, 511499)\t1.0\n",
      "  (2175892, 398069)\t-1.5\n",
      "  (2175892, 143769)\t-1.0\n",
      "  (2175892, 117252)\t-1.5\n",
      "  (2175892, 82618)\t1.5\n",
      "  (2175892, 55103)\t1.5\n",
      "  (2175892, 45186)\t-1.0\n",
      "  (2175893, 512177)\t1.5\n",
      "  (2175893, 386083)\t-1.0\n",
      "  (2175893, 280419)\t1.5\n",
      "  (2175893, 214066)\t1.0\n",
      "  (2175893, 188737)\t1.5\n",
      "  (2175893, 179021)\t1.5\n",
      "  (2175893, 177884)\t-1.0\n",
      "  (2175893, 82618)\t1.5\n",
      "  (2175893, 9643)\t1.0\n",
      "[481.3867721557617] Create sparse merge completed\n",
      "(2175894, 2558294)\n",
      "[600.2765281200409] Train FTRL completed\n",
      "FTRL Dev MSLE: 0.461336881348\n",
      "[600.9440822601318] Predict FTRL completed\n",
      "[1973.0522990226746] Train ridge v2 completed\n",
      "FM_FTRL dev RMSLE: 0.429634509195\n",
      "[1995.2061841487885] Predict FM_FTRL completed\n",
      "(2175894, 65499)\n",
      "(2175894, 65499)\n"
     ]
    }
   ],
   "source": [
    "# Based on Bojan: https://www.kaggle.com/tunguz/wordbatch-ftrl-fm-lgb-lbl-0-42506\n",
    "\n",
    "import re\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "from time import gmtime,strftime\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordBag,WordHash\n",
    "from wordbatch.models import FTRL,FM_FTRL\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "# Function to handle the missing data.\n",
    "def handle_missing_data(comb):\n",
    "    comb['first_category'].fillna(value='missing_category',inplace=True)\n",
    "    comb['second_category'].fillna(value='missing_category',inplace=True)\n",
    "    comb['third_category'].fillna(value='missing_category',inplace=True)\n",
    "    comb['item_description'].fillna(value='missing_description',inplace=True)\n",
    "    comb['brand_name'].fillna(value='missing_brand',inplace=True)\n",
    "    comb['name'].fillna(value='missing_name',inplace=True)\n",
    "    #return comb\n",
    "\n",
    "# Function to split the category into sub-categories\n",
    "def custom_split(description):\n",
    "    try:\n",
    "        return description.split(\"/\")\n",
    "    except:\n",
    "        return [\"No Label\",\"No Label\",\"No Label\"]\n",
    "\n",
    "# Function to change the data type of the categorical variable.\n",
    "def convert_to_categorical(comb):\n",
    "    comb['first_category'] = comb['first_category'].astype('category')\n",
    "    comb['second_category'] = comb['second_category'].astype('category')\n",
    "    comb['third_category'] = comb['third_category'].astype('category')\n",
    "    comb['item_condition_id'] = comb['item_condition_id'].astype('category')\n",
    "\n",
    "    \n",
    "# Function to filter the dataset to feed to the model.\n",
    "def filtering_dataset(comb):\n",
    "    popular_brand = comb['brand_name'].value_counts().loc[lambda x: x.index !='missing_brand'].index[:4500]\n",
    "    comb.loc[~comb['brand_name'].isin(popular_brand),'brand_name'] = 'missing'\n",
    "    popular_first_category = comb['first_category'].value_counts().loc[lambda x: x.index!='missing_category'].index[0:1250]\n",
    "    comb.loc[~comb['first_category'].isin(popular_first_category),'first_category'] ='missing'\n",
    "    popular_second_category = comb['second_category'].value_counts().loc[lambda x: x.index!='missing_category'].index[0:1250]\n",
    "    comb.loc[~comb['second_category'].isin(popular_second_category),'third_category'] = 'missing'\n",
    "    popular_third_category = comb['third_category'].value_counts().loc[lambda x: x.index!='missing_category'].index[0:1250]\n",
    "    comb.loc[~comb['third_category'].isin(popular_third_category),'third_category'] = 'missing'\n",
    "\n",
    "stopwords = {x:1 for x in stopwords.words('english')}\n",
    "non_alphanums = re.compile(u'[^A-Za-z0-9]+')\n",
    "# def normalize_text(text):\n",
    "#     return u\" \".join([x for x in [y for y in non_aplhanums.sub(' ',text).lower().strip().split(\" \")]\\\n",
    "#                      if len(x) > 1 and x not in stopwords])\n",
    "\n",
    "# Function to normalize the text\n",
    "def normalize_text(text):\n",
    "    return u\" \".join(\n",
    "        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n",
    "         if len(x) > 1 and x not in stopwords])\n",
    "\n",
    "# Function to compute the Root Mean Squared Logarithmic Error.\n",
    "def rmsle(y_act,y_pred):\n",
    "    assert len(y_act) == len(y_pred)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y_act) - np.log1p(y_pred),2)))\n",
    "\n",
    "\n",
    "# Function for implementing the business logic.\n",
    "def main_function():\n",
    "    start_time = time.time()\n",
    "    print(start_time)\n",
    "    print(strftime(\"%Y-%m-%d %H:%M:%S\",gmtime()))\n",
    "    mercari_data_train = pd.read_table('../input/train.tsv',engine='c')\n",
    "    mercari_data_test = pd.read_table('../input/test.tsv',engine='c')\n",
    "    print('[{}] Finished to load the data'.format(time.time() - start_time))\n",
    "    print('Shape of training Data',mercari_data_train.shape)\n",
    "    print('Shape of testing Data',mercari_data_test.shape)\n",
    "    nrow_test = mercari_data_train.shape[0]\n",
    "    dftt = mercari_data_train[(mercari_data_train.price < 1.0)]\n",
    "    mercari_data_train = mercari_data_train.drop(mercari_data_train[(mercari_data_train.price < 1.0)].index)\n",
    "    del dftt['price']\n",
    "    nrow_train = mercari_data_train.shape[0]\n",
    "    y = np.log1p(mercari_data_train[\"price\"])\n",
    "    print('Fields of training dataset',mercari_data_train.columns)\n",
    "    print('Fields of testing dataset',mercari_data_test.columns) \n",
    "    comb: pd.DataFrame = pd.concat([mercari_data_train,dftt,mercari_data_test])\n",
    "    print('Shape of training data:',comb.shape)\n",
    "    submission:pd.Dataframe = mercari_data_test[['test_id']]\n",
    "    #comb = comb[comb['category_name'].notnull()]\n",
    "    comb['first_category'],comb['second_category'],comb['third_category'] = zip(*comb['category_name'].apply(lambda x: custom_split(x)))\n",
    "    handle_missing_data(comb)\n",
    "    comb.drop(['category_name'],axis=1,inplace=True)\n",
    "    print('[{}] Split categories complete and original dropped'.format(time.time() - start_time))\n",
    "    filtering_dataset(comb)\n",
    "    convert_to_categorical(comb)\n",
    "    print(comb.dtypes)\n",
    "    comb = comb[comb['name'].notnull()]\n",
    "    print(comb['name'].head(n=500))\n",
    "    word_batch = wordbatch.WordBatch(normalize_text,extractor=(WordBag,{\"hash_ngrams\":2,\"hash_ngrams_weights\":[1.5,1.0],\"hash_size\":2**29,\"norm\":None,\"tf\":\"binary\",\"idf\":None}),procs=4)\n",
    "    word_batch.dictionary_freeze=True\n",
    "    #comb.head(n=5)\n",
    "    #comb.columns.values\n",
    "    X_name = word_batch.fit_transform(comb['name'])\n",
    "    X_name = X_name[:,np.where(X_name.getnnz(axis=0) > 1)[0]]\n",
    "    del(word_batch)\n",
    "    word_batch = CountVectorizer()\n",
    "    X_first_category = word_batch.fit_transform(comb['first_category'])\n",
    "    X_second_category = word_batch.fit_transform(comb['second_category'])\n",
    "    X_third_category = word_batch.fit_transform(comb['third_category'])\n",
    "    print('[{}] Count Vectorize categories completed.'.format(time.time() - start_time))\n",
    "    # Word Batch for item description\n",
    "    word_batch = wordbatch.WordBatch(normalize_text,extractor=(WordBag,{\"hash_ngrams\":2,\"hash_ngrams_weights\":[1.5,1.0],\"hash_size\":2**29,\"norm\":\"l2\",\"tf\":1.0,\"idf\":None,}),procs=8)\n",
    "    word_batch.dictionary_freeze=True\n",
    "    X_description = word_batch.fit_transform(comb['item_description'])\n",
    "    del(word_batch)\n",
    "    X_description = X_description[:,np.where(X_description.getnnz(axis=0)>1)[0]]\n",
    "    print('[{}] Vectorize item_description completed.'.format(time.time() - start_time))\n",
    "    lb = LabelBinarizer(sparse_output=True)\n",
    "    X_brand = lb.fit_transform(comb['brand_name'])\n",
    "    print('[{}] Label Binarize brand name completed.'.format(time.time() - start_time))\n",
    "    X_dummies = csr_matrix(pd.get_dummies(comb[['item_condition_id','shipping']],sparse=True).values)\n",
    "    print('[{}] Get dummies on item condition and shipping done.'.format(time.time() - start_time))\n",
    "    print(X_dummies.shape,X_description.shape,X_brand.shape,X_first_category.shape,X_second_category.shape,X_third_category.shape,X_name)\n",
    "    sparse_merge = hstack((X_dummies,X_description,X_brand,X_first_category,X_second_category,X_third_category,X_name)).tocsr()\n",
    "    print('[{}] Create sparse merge completed'.format(time.time() - start_time))\n",
    "    print(sparse_merge.shape)\n",
    "    sparse_merge = sparse_merge[:,np.where(sparse_merge.getnnz(axis=0) > 100)[0]]\n",
    "    X = sparse_merge[:nrow_train]\n",
    "    X_test = sparse_merge[nrow_test:]\n",
    "    #print(sparse_merge.head(n=5))\n",
    "    \n",
    "    gc.collect()\n",
    "    train_X,train_y = X,y\n",
    "    train_X,valid_X,train_y,valid_y = train_test_split(X,y,test_size=0.05,random_state=100)\n",
    "    model  = FTRL(alpha=0.01,beta=0.1,L1=0.00001,L2=1.0,D=sparse_merge.shape[1],iters=50,inv_link=\"identity\",threads=1)\n",
    "    model.fit(train_X,train_y)\n",
    "    print('[{}] Train FTRL completed'.format(time.time() - start_time))\n",
    "    preds = model.predict(X = valid_X)\n",
    "    print(\"FTRL Dev MSLE:\",rmsle(np.expm1(valid_y),np.expm1(preds)))\n",
    "    \n",
    "    predsF = model.predict(X_test)\n",
    "    print('[{}] Predict FTRL completed'.format(time.time() - start_time))\n",
    "    \n",
    "    model = FM_FTRL(alpha=0.01,beta=0.01,L1=0.00001,L2=0.1,D=sparse_merge.shape[1],alpha_fm=0.01,L2_fm=0.0,init_fm=0.01,D_fm=200,e_noise=0.0001,iters=20,inv_link='identity',threads=4)\n",
    "    model.fit(train_X,train_y)\n",
    "    print('[{}] Train ridge v2 completed'.format(time.time() - start_time))\n",
    "    preds = model.predict(X=valid_X)\n",
    "    print(\"FM_FTRL dev RMSLE:\",rmsle(np.expm1(valid_y),np.expm1(preds)))\n",
    "    \n",
    "    predsFM = model.predict(X_test)\n",
    "    print('[{}] Predict FM_FTRL completed'.format(time.time() - start_time))\n",
    "    \n",
    "    params = {'learning_rate':0.2,'application':'regression','max_depth':4,'num_leaves':15,'verbosity':-1,'metric':'RMSE','data_random_seed':1,'bagging_fraction':0.6,'bagging_freq':5,'feature_fraction':0.65,'nthread':4,'min_data_in_leaf':100,'max_bin':16}\n",
    "    \n",
    "    # Remove features with document frequency <=100\n",
    "    print(sparse_merge.shape)\n",
    "    sparse_merge = sparse_merge[:,np.where(sparse_merge.getnnz(axis=0) > 100)[0]]\n",
    "    X = sparse_merge[:nrow_train]\n",
    "    X_test = sparse_merge[nrow_test:]\n",
    "    print(sparse_merge.shape)\n",
    "    train_X,train_y = X,y\n",
    "    train_X,valid_X,train_y,valid_y = train_test_split(X,y,test_size=0.05,random_state=100)\n",
    "    d_train = lgb.Dataset(train_X,label=train_y)\n",
    "    watch_list = [d_train]\n",
    "    d_valid = lgb.Dataset(valid_X,label=valid_y)\n",
    "    watch_list = [d_train,d_valid]\n",
    "    model = lgb.train(params,train_set=d_train,num_boost_round=200,valid_sets=watch_list,early_stopping_rounds=50,verbose_eval=20)\n",
    "    preds = model.predict(valid_X)\n",
    "    print(\"LGB dev RMSLE:\",rmsle(np.expm1(valid_y),np.expm1(preds)))\n",
    "    predsL = model.predict(X_test)\n",
    "    print('[{}] Predict LGB completed.'.format(time.time() - start_time))\n",
    "    preds = (predsF*0.18 + predsL*0.27 + predsFM*0.55)\n",
    "    submission['price'] = np.expm1(preds)\n",
    "    submission.to_csv(\"wordbatch_ftrl_fm_lgb.csv\",index=False)\n",
    "    \n",
    "main_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
